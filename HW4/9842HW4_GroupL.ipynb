{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH9842 HW4\n",
    "## Group L\n",
    "## Yang Li, Jimmy Jusuf, Zhirong Zhang, Tiangang Zhang\n",
    "## December 19, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\displaystyle{f(\\mathbf{x})={-\\!}\\sum_{i=1}^n \\log x_i}$ be the objective function, $\\mathbf{x}\\in \\mathbb{R}^n$.\n",
    "\n",
    "## (i)\n",
    "We want to solve\n",
    "$$\n",
    "\\min\\ f(\\mathbf{x}), \\text{ subject to a single equality constraint } h(\\mathbf{x})=\\sum_{i=1}^n x_i -1 = 0\n",
    "$$\n",
    "\n",
    "Define Lagrange function\n",
    "\n",
    "$$\n",
    "L(\\mathbf{x},\\lambda) = f(\\mathbf{x}) + \\lambda\\, h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Any optimal points $\\mathbf{x}^*$ must satisfy KKT conditions:\n",
    "\\begin{align}\n",
    "\\nabla L_x = \\nabla f(\\mathbf{x}^*) + \\lambda \\nabla h(\\mathbf{x}^*) = 0 \\\\\n",
    "h(\\mathbf{x}^*) = 0 \\\\\n",
    "\\lambda\\geq 0 \\\\\n",
    "\\end{align}\n",
    "We conclude\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = 0 \n",
    "  \\Rightarrow -\\frac{1}{x_i} + \\lambda = 0 \n",
    "  \\Rightarrow x_i = \\frac 1\\lambda\n",
    "$$\n",
    "$$\n",
    "h(\\mathbf{x})=0 \\Rightarrow \\sum_{i=1}^n x_i = 1 \n",
    "  \\Rightarrow \\sum_{i=1}^n \\frac1\\lambda = 1\n",
    "  \\Rightarrow n\\frac1\\lambda = 1 \n",
    "  \\Rightarrow \\lambda = n\n",
    "  \\Rightarrow x_i = \\frac 1n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii)\n",
    "Given an optimization problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "  \\text{minimize } & f(\\mathbf{x})={-\\!}\\sum_{i=1}^n \\log x_i \\\\\n",
    "  \\text{subject to } & \\mathbf{Ax}-\\mathbf{b}=0\n",
    "\\end{array}\n",
    "$$\n",
    "Similar to (i), define Lagrange function\n",
    "$$\n",
    "L(\\mathbf{x},\\boldsymbol\\lambda) = f(\\mathbf{x}) + (\\mathbf{Ax} - \\mathbf{b})^T\\boldsymbol\\lambda,\n",
    "  \\quad \\mathbf{A}\\in \\mathbb{R}^{p\\times n}, \n",
    "  \\mathbf{b}\\in\\mathbb{R}^p, \n",
    "  \\boldsymbol\\lambda\\in\\mathbb{R}^p\n",
    "$$\n",
    "\n",
    "The Lagrange dual function is\n",
    "$$\n",
    "q(\\boldsymbol\\lambda) = \\inf_{\\mathbf{x}\\in\\mathcal{D}}L(\\mathbf{x},\\boldsymbol\\lambda)\n",
    "$$\n",
    "\n",
    "Holding $\\boldsymbol\\lambda$ fixed, this is a convex function of $x$. The infimum with respect \n",
    "to $x$ is achieved when $\\displaystyle{\\frac{\\partial L}{\\partial x_i}}$ are zero.\n",
    "\n",
    "Here, $A_{ji}$ is the $j$-th row, $i$-th col element in $\\mathbf{A}$, $i\\in 1\\ldots n$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = 0 \n",
    "  \\Rightarrow  -\\frac{1}{x_i} + \\lambda_j\\sum_{j=1}^p A_{ji} = 0\n",
    "  \\Rightarrow x_i = \\frac{1}{\\lambda\\sum_{j=1}^p A_{ji}} = \\frac{1}{(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_i}\n",
    "$$\n",
    "\n",
    "Also we have,\n",
    "$$\\mathbf{Ax} = \n",
    "A\\left(\\begin{array}{c}\\frac{1}{\\lambda \\sum_j A_{j1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{1}{\\lambda \\sum_j A_{jn}}\\\\\n",
    "\\end{array}\\right) = \n",
    "\\left(\n",
    "\\begin{array}{c}\\sum_i\\frac{ A_{1i}}{\\lambda \\sum_j A_{ji}}\\\\\n",
    "\\vdots\\\\\n",
    "\\sum_i\\frac{ A_{ni}}{\\lambda \\sum_j A_{ji}}\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "Thus we have\n",
    "$$\n",
    "\\mathbf{x}^T\\mathbf{A}^{\\!T}\\boldsymbol\\lambda = \\left(\n",
    "\\begin{array}{c}\\sum_i\\frac{ A_{1i}}{\\lambda \\sum_j A_{ji}}&\n",
    "\\cdots&\n",
    "\\sum_i\\frac{ A_{ni}}{\\lambda \\sum_j A_{ji}}\\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cc}\\lambda\\\\ \n",
    "\\vdots\\\\\n",
    "\\lambda \\end{array}\\right) = n\n",
    "$$\n",
    "Substituting into $q(\\boldsymbol\\lambda)$\n",
    "$$\n",
    "q(\\boldsymbol\\lambda) = \\inf_{\\mathbf{x}\\in\\mathcal{D}}L(\\mathbf{x},\\boldsymbol\\lambda) = \n",
    "  \\inf_{\\mathbf{x}\\in\\mathcal{D}}\\left( -\\!\\sum_{i=1}^n \\log x_i + (\\mathbf{Ax} - \\mathbf{b})^T\\boldsymbol\\lambda\\right) =\n",
    "  -\\mathbf{b}^T\\boldsymbol\\lambda - \\sum_{i = 1}^{n} \\log\\frac1{(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_i} + n\n",
    "$$\n",
    "i.e. the dual problem is\n",
    "$$\n",
    "\\text{maximize}\\quad q(\\boldsymbol\\lambda) = -\\mathbf{b}^T\\boldsymbol\\lambda +\\sum_{i = 1}^{n} \n",
    "  \\log {(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_i} + n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iii)\n",
    "Already solved in (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iv)\n",
    "By Newton's method, we need to calculate the matrix equation\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\nabla^2 f &A^T\\\\\n",
    "A& 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-\\Delta x\\\\\n",
    "\\lambda\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "It's easy to see that \n",
    "\n",
    "$$\n",
    "\\nabla f = (-\\frac{1}{x_1},\\cdots, -\\frac{1}{x_n})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 f = \\operatorname{diag}(\\frac{1}{x_1^2},\\cdots, \\frac{1}{x_n^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (v)\n",
    "To solve the dual problem, we use the following formula for gradient and hessian of\n",
    "$q(\\boldsymbol\\lambda)$.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\!\\lambda}\\,q=-\\mathbf{b}+\\mathbf{A}y \\text{ where }\n",
    "  y=(1/(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_1,\\ldots,\n",
    "  1/(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_n) \\\\\n",
    "\\nabla^2_{\\!\\lambda}\\,q=-\\mathbf{ADA}^{\\!T}, \\quad D \\text{ is diagonal, with }\n",
    "  D_{ii}=(\\mathbf{A}^{\\!T}\\boldsymbol\\lambda)_i^{-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# global variables to keep track of search history\n",
    "x_hist = []\n",
    "fx_hist = []  # objective function value\n",
    "dfx_hist = []  # derivative of objective function / gradient vector at x\n",
    "hfx_hist = []  # second derivative of objective function / hessian matrix at x\n",
    "dx_hist = []  # Newton step / search direction\n",
    "epsilon_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect(a):\n",
    "    # turn input into a column vector, i.e. a.shape=(numrow,1)\n",
    "    if isvect(a): return a  # do nothing if input is already a vector       \n",
    "    av=np.array(a)[:,None]\n",
    "    #print(av.shape)\n",
    "    assert av.shape==(len(a),1)\n",
    "    return av\n",
    "\n",
    "def isvect(x):\n",
    "    # check if x is a vect\n",
    "    try:\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        if (x.ndim!=2): return False\n",
    "        (nr,nc) = x.shape\n",
    "        return nc==1\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sumcol(v):\n",
    "    # input is a vector (assume column vector, but works with row vector too)\n",
    "    # return the sum of elements\n",
    "    assert isinstance(v, np.ndarray)\n",
    "    return np.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "a=vect([1,2,3])\n",
    "b=np.array([4,5,6])\n",
    "print(isvect(a), isvect(b)) # output: True False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "# Newton method for unconstrained problem\n",
    "def NewtonUnconstrainedSolver(x0, alpha, beta, tol, objfun, objfun_grad, objfun_hess):\n",
    "    assert np.isscalar(alpha) and np.isscalar(beta) and np.isscalar(tol)\n",
    "    epsilon =  100 # get the Newton decrement\n",
    "    gradtol = 1e-5\n",
    "    count = 0\n",
    "    x = x0\n",
    "    while epsilon > tol and count < 500:      \n",
    "        count = count + 1\n",
    "        funcval, gradvec, hessmat = objfun(x), objfun_grad(x), objfun_hess(x)\n",
    "        print(\"x=\", x, \"\\nf(x)=\", funcval, \"\\ngrad(f(x))\", gradvec, \"\\nhess(f(x))\", hessmat)\n",
    "        newton_step = -1*np.matmul(np.linalg.inv(hessmat), gradvec)\n",
    "        print(\"newton_step\", newton_step)\n",
    "        descmag = np.squeeze(gradvec.T.dot(newton_step)) # check descent magnitude\n",
    "        print(\"descent magnitude\", descmag)\n",
    "        # using backtracking line search\n",
    "        back_ct=0\n",
    "        t = 1\n",
    "        # while -np.log(x + t * dx).sum() >= -np.log(x).sum() - alpha * t * dfx.T.dot(dx):\n",
    "        while True:\n",
    "            new_x = x+t*newton_step\n",
    "            print(\"new_x\", new_x)\n",
    "            new_funcval = objfun(new_x)\n",
    "            print(new_funcval, alpha, t, descmag)\n",
    "            if new_funcval < (funcval-alpha*t*descmag): break\n",
    "            t = t * beta\n",
    "            back_ct+=1\n",
    "            if back_ct>50: break\n",
    "        if (abs(descmag)<gradtol): break\n",
    "        # update the parameters\n",
    "        last_x=x\n",
    "        x = new_x\n",
    "        fnew, gradvec, hessmat = new_funcval, objfun_grad(x), objfun_hess(x)\n",
    "        # epsilon =  2*np.sqrt(-np.log(last_x).sum()+np.log(x).sum())\n",
    "        epsilon =  2*np.sqrt(objfun(last_x)-objfun(x))\n",
    "    return x,epsilon,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "# class\n",
    "class NewtonSolver:\n",
    "    # Class attribute\n",
    "    objfunc_val = 0;\n",
    "    gradvec = 0;\n",
    "    hess = 0;\n",
    "\n",
    "    # Initializer / Instance attributes\n",
    "    def __init__(self):\n",
    "        None\n",
    "\n",
    "    # instance method\n",
    "    def objfunc(self, x):\n",
    "        return \"{} is {} years old\".format(self.name, self.age)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return \"{} says {}\".format(self.name, sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "# dual function optimizer\n",
    "def nmdual(x0, A, b, alpha, beta, tol):\n",
    "    def func(x):\n",
    "        # implement the dual function\n",
    "        (nr,nc) = A.shape\n",
    "        atl=np.matmul(A.T,x)\n",
    "        sumlog = sumcol(np.log(x))\n",
    "        return -1*(-np.dot(b.T,x)+sumlog+nc)\n",
    "    def grad(x):\n",
    "        # return the gradient of objective function\n",
    "        g=np.matmul(A, inv(np.matmul(A.T,x)))\n",
    "        return -1*(vect(-b)+g)\n",
    "    def hess(x):\n",
    "        # return the hessian of the objective function\n",
    "        atl=np.matmul(A.T,x)\n",
    "        atlinv=inv(atl)\n",
    "        Dii=np.diag(atlinv**2)\n",
    "        D=np.diag(np.matmul(A.T,x))\n",
    "        return -1*((-A @ D) @ A.T)\n",
    "    # compute A' lambda\n",
    "    #atl=np.matmul(A.T,x0)\n",
    "    #atlinv=inv(atl)\n",
    "    #Dii=np.diag(atlinv**2)\n",
    "    return NewtonUnconstrainedSolver(x0, alpha, beta, tol, func, grad, hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "# test code for Newton unconstrained optimization problem\n",
    "def nmprimal(x0, alpha, beta, tol):\n",
    "    def func(x):\n",
    "        # implement objective function f(x) = -sum(log(x_i))\n",
    "        return sumcol(-np.log(x))\n",
    "    def grad(x):\n",
    "        # evaluate the gradient of objective function at point x\n",
    "        return vect(-1/x)\n",
    "    def hess(x):\n",
    "        # evaluate the hessian of the objective function at point x\n",
    "        return np.diagflat(1/x**2)\n",
    "    return NewtonUnconstrainedSolver(x0, alpha, beta, tol, func, grad, hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "tol = 0.0001\n",
    "x0=vect([0.1,0.4,0.3])\n",
    "A=np.hstack((vect([1,1,1]),vect([2,2,2]),vect([3,3,3])))\n",
    "b=vect([1,1,1])\n",
    "#nmdual(x0,alpha,beta,tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewtonMethod(A, x0, alpha, beta, tol):\n",
    "    \"\"\"\n",
    "    Implement Newton's method optimization with equality constraints.\n",
    "    Input:\n",
    "      A is the coefficient of the matrix that represents equality constraint (Ax-b)=0\n",
    "      alpha, beta are parameters for backtracking line search\n",
    "      tol, the algorithm terminates if |f(x)-f(last_x)| < tol\n",
    "      Function f(), f_grad(), f_hess() represent the objective function to be minimized,\n",
    "        the gradient and the hessian\n",
    "    Output:\n",
    "      x where the minimum is found\n",
    "      epsilon the value of |f(x)-f(last_x)| from the final iteration\n",
    "      count, the number of iterations\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        # evaluate objective function at x\n",
    "        return -np.log(x).sum()\n",
    "    def f_grad(x):\n",
    "        # return the gradient vector at f(x)\n",
    "        return -1 / x\n",
    "    def f_hess(x):\n",
    "        # return the hessian matrix at f(x)\n",
    "        return np.diag(1/x**2)\n",
    "    (row, col) = A.shape\n",
    "    x = x0\n",
    "    H = f_hess(x) # H is hessian of f(x)\n",
    "    upper = np.column_stack((H, A.T)) # [Hessian, A^T]\n",
    "    lower = np.column_stack((A, np.zeros((row, row)))) # [A, 0]\n",
    "    KKT = np.row_stack((upper, lower)) # this the the matrix on the left\n",
    "    dfx = f_grad(x) # this is - nabla f(x)\n",
    "    RHS = np.append(dfx, np.zeros(row)) # right hand side\n",
    "    epsilon =  100 # get the Newton decrement\n",
    "    count = 0\n",
    "    # clear search history\n",
    "    x_hist.clear()\n",
    "    fx_hist.clear()  # objective function value\n",
    "    dfx_hist.clear()  # derivative of objective function / gradient vector at x\n",
    "    hfx_hist.clear()  # second derivative of objective function / hessian matrix at x\n",
    "    dx_hist.clear()  # Newton step / search direction\n",
    "    epsilon_hist.clear()\n",
    "    while epsilon > tol and count < 500:\n",
    "        \n",
    "        count = count + 1\n",
    "        # save the search paths\n",
    "        x_hist.append(x)\n",
    "        fx_hist.append(f(x))\n",
    "        dfx_hist.append(dfx)\n",
    "        hfx_hist.append(H)\n",
    "        \n",
    "        LHS = np.squeeze(np.array(inv(KKT).dot(RHS)))\n",
    "        dx = -np.array(LHS)[:col]; dx_hist.append(dx)\n",
    "        t = 1\n",
    "        # using backtracking line search\n",
    "        back_ct=0\n",
    "        fval = f(x)\n",
    "        print(fval)\n",
    "        while f(x + t * dx) >= fval - alpha * t * dfx.T.dot(dx):\n",
    "            t = t * beta\n",
    "            back_ct+=1\n",
    "            if back_ct>50:\n",
    "                break\n",
    "        # update the parameters\n",
    "        last_x=x\n",
    "        x = x + t * dx\n",
    "        H = f_hess(x)\n",
    "        upper = np.column_stack((H, A.T))\n",
    "        lower = np.column_stack((A, np.zeros((row, row))))\n",
    "        KKT = np.row_stack((upper, lower))\n",
    "        dfx = f_grad(x)\n",
    "        RHS = np.append(dfx, np.zeros(row))\n",
    "        epsilon =  abs(f(last_x) - f(x)); epsilon_hist.append(epsilon)\n",
    "        #print(dfx.T.dot(dx))\n",
    "    return x,epsilon,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "def NewtonMethodDual(A, b, x0, alpha, beta, tol):\n",
    "    \"\"\"\n",
    "    Implement Newton's method optimization with equality constraints.\n",
    "    Input:\n",
    "      A is the coefficient of the matrix that represents equality constraint (Ax-b)=0\n",
    "      alpha, beta are parameters for backtracking line search\n",
    "      tol, the algorithm terminates if |f(x)-f(last_x)| < tol\n",
    "      Function f(), f_grad(), f_hess() represent the objective function to be minimized,\n",
    "        the gradient and the hessian\n",
    "    Output:\n",
    "      x where the minimum is found\n",
    "      epsilon the value of |f(x)-f(last_x)| from the final iteration\n",
    "      count, the number of iterations\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        \"\"\" evaluate objective function at x \"\"\"\n",
    "        atx = A.T @ x\n",
    "        btxm = -b.T @ x\n",
    "        n = len(atx)\n",
    "        return flipsign*(btxm+np.log(atx).sum()+n)\n",
    "    def f_grad(x):\n",
    "        \"\"\" return the gradient vector at f(x) \"\"\"\n",
    "        atx = A.T @ x\n",
    "        y = 1/atx\n",
    "        Ay = A @ y\n",
    "        return flipsign*(-b+Ay)\n",
    "    def f_hess(x):\n",
    "        \"\"\" return the hessian matrix at f(x) \"\"\"\n",
    "        atx = A.T @ x\n",
    "        y = 1/atx**2\n",
    "        D = np.diagflat(y)\n",
    "        ADAm = -A @ D @ A.T\n",
    "        return flipsign*(ADAm)\n",
    "    \n",
    "    flipsign = -1\n",
    "    (row, col) = A.shape\n",
    "    x = x0\n",
    "    H = f_hess(x) # H is hessian of f(x)\n",
    "    upper = np.column_stack((H, A.T)) # [Hessian, A^T]\n",
    "    lower = np.column_stack((A, np.zeros((row, row)))) # [A, 0]\n",
    "    KKT = np.row_stack((upper, lower)) # this the the matrix on the left\n",
    "    dfx = f_grad(x) # this is - nabla f(x)\n",
    "    RHS = np.append(dfx, np.zeros(row)) # right hand side\n",
    "    epsilon =  100 # get the Newton decrement\n",
    "    count = 0\n",
    "    # clear search history\n",
    "    x_hist.clear()\n",
    "    fx_hist.clear()  # objective function value\n",
    "    dfx_hist.clear()  # derivative of objective function / gradient vector at x\n",
    "    hfx_hist.clear()  # second derivative of objective function / hessian matrix at x\n",
    "    dx_hist.clear()  # Newton step / search direction\n",
    "    epsilon_hist.clear()\n",
    "    while epsilon > tol and count < 500:\n",
    "        \n",
    "        count = count + 1\n",
    "        # save the search paths\n",
    "        x_hist.append(x)\n",
    "        fx_hist.append(f(x))\n",
    "        dfx_hist.append(dfx)\n",
    "        hfx_hist.append(H)\n",
    "        \n",
    "        LHS = np.squeeze(np.array(inv(KKT).dot(RHS)))\n",
    "        dx = -np.array(LHS)[:col]; dx_hist.append(dx)\n",
    "        t = 1\n",
    "        # using backtracking line search\n",
    "        back_ct=0\n",
    "        fval = f(x)\n",
    "        while f(x + t * dx) >= fval - alpha * t * dfx.T.dot(dx):\n",
    "            t = t * beta\n",
    "            back_ct+=1\n",
    "            if back_ct>50:\n",
    "                break\n",
    "        # update the parameters\n",
    "        last_x=x\n",
    "        x = x + t * dx\n",
    "        H = f_hess(x)\n",
    "        upper = np.column_stack((H, A.T))\n",
    "        lower = np.column_stack((A, np.zeros((row, row))))\n",
    "        KKT = np.row_stack((upper, lower))\n",
    "        dfx = f_grad(x)\n",
    "        RHS = np.append(dfx, np.zeros(row))\n",
    "        epsilon =  abs(f(last_x) - f(x)); epsilon_hist.append(epsilon)\n",
    "        #print(dfx.T.dot(dx))\n",
    "    return x,epsilon,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "# beta = 0.5\n",
    "# tol = 1e-12\n",
    "# A=np.array([[1,1,1,1,1]]) # define 1x5 matrix (p=1, n=5), single constraint sum(x_i)=1\n",
    "# b=vect([1])\n",
    "# x=np.array([2,2,2,2,2])\n",
    "# atx=A.T @ l\n",
    "# y=1/atx**2\n",
    "# D=np.diagflat(y)\n",
    "# -A @ D @ A.T\n",
    "# #NewtonMethodDual(A, b, l, alpha, beta, tol)\n",
    "# NewtonMethod(A, x, alpha, beta, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "tol = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with a small test case\n",
    "# n=3 (dimension)\n",
    "# p=1 (constrain sum(x_i)=1)\n",
    "# x0 must start in the feasible set\n",
    "x0=np.array([0.2,0.5,0.3])  # note the element-sum of x0 is 1\n",
    "A=np.array([[1,1,1]])\n",
    "b=np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.506557897319982\n",
      "3.3072267512534124\n",
      "3.2958912736500254\n",
      "3.295836866997819\n",
      "3.29583686600433\n",
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.33333333, 0.33333333, 0.33333333]), 4.440892098500626e-16, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "NewtonMethod(A, x0, alpha, beta, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.2, 0.5, 0.3]),\n",
       " array([0.29473684, 0.34210526, 0.36315789]),\n",
       " array([0.3307705 , 0.33567068, 0.33355882]),\n",
       " array([0.33332553, 0.3333292 , 0.33334528]),\n",
       " array([0.33333333, 0.33333333, 0.33333333])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 0.9999999999999998]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the constraint sum(x_i)=1 holds at all times during the search\n",
    "[sum(x) for x in x_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.993311e-01\n",
       "1    1.133548e-02\n",
       "2    5.440665e-05\n",
       "3    9.934888e-10\n",
       "4    4.440892e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts=pd.Series(epsilon_hist)\n",
    "plt=ts.plot(logy=True)\n",
    "plt.set_xlabel('iteration')\n",
    "plt.set_ylabel('epsilon')\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 1.1102230246251565e-16, 2.220446049250313e-16]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute l1-norm of x_hist[0] with every elem of x_hist[0..n]\n",
    "# the output should be practically zero since the algorithm ensures x stays feasible during the search\n",
    "[sum(A@(x_hist[0] - x)) for x in x_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.array([1 for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cst(j,i):\n",
    "    if j==i-1 or j==i:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2=np.array([[cst(j,i) for j in range(500)] for i in range(1,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.row_stack((A1,A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.5*i+1 for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1. ,   1.5,   2. ,   2.5,   3. ,   3.5,   4. ,   4.5,   5. ,\n",
       "         5.5,   6. ,   6.5,   7. ,   7.5,   8. ,   8.5,   9. ,   9.5,\n",
       "        10. ,  10.5,  11. ,  11.5,  12. ,  12.5,  13. ,  13.5,  14. ,\n",
       "        14.5,  15. ,  15.5,  16. ,  16.5,  17. ,  17.5,  18. ,  18.5,\n",
       "        19. ,  19.5,  20. ,  20.5,  21. ,  21.5,  22. ,  22.5,  23. ,\n",
       "        23.5,  24. ,  24.5,  25. ,  25.5,  26. ,  26.5,  27. ,  27.5,\n",
       "        28. ,  28.5,  29. ,  29.5,  30. ,  30.5,  31. ,  31.5,  32. ,\n",
       "        32.5,  33. ,  33.5,  34. ,  34.5,  35. ,  35.5,  36. ,  36.5,\n",
       "        37. ,  37.5,  38. ,  38.5,  39. ,  39.5,  40. ,  40.5,  41. ,\n",
       "        41.5,  42. ,  42.5,  43. ,  43.5,  44. ,  44.5,  45. ,  45.5,\n",
       "        46. ,  46.5,  47. ,  47.5,  48. ,  48.5,  49. ,  49.5,  50. ,\n",
       "        50.5,  51. ,  51.5,  52. ,  52.5,  53. ,  53.5,  54. ,  54.5,\n",
       "        55. ,  55.5,  56. ,  56.5,  57. ,  57.5,  58. ,  58.5,  59. ,\n",
       "        59.5,  60. ,  60.5,  61. ,  61.5,  62. ,  62.5,  63. ,  63.5,\n",
       "        64. ,  64.5,  65. ,  65.5,  66. ,  66.5,  67. ,  67.5,  68. ,\n",
       "        68.5,  69. ,  69.5,  70. ,  70.5,  71. ,  71.5,  72. ,  72.5,\n",
       "        73. ,  73.5,  74. ,  74.5,  75. ,  75.5,  76. ,  76.5,  77. ,\n",
       "        77.5,  78. ,  78.5,  79. ,  79.5,  80. ,  80.5,  81. ,  81.5,\n",
       "        82. ,  82.5,  83. ,  83.5,  84. ,  84.5,  85. ,  85.5,  86. ,\n",
       "        86.5,  87. ,  87.5,  88. ,  88.5,  89. ,  89.5,  90. ,  90.5,\n",
       "        91. ,  91.5,  92. ,  92.5,  93. ,  93.5,  94. ,  94.5,  95. ,\n",
       "        95.5,  96. ,  96.5,  97. ,  97.5,  98. ,  98.5,  99. ,  99.5,\n",
       "       100. , 100.5, 101. , 101.5, 102. , 102.5, 103. , 103.5, 104. ,\n",
       "       104.5, 105. , 105.5, 106. , 106.5, 107. , 107.5, 108. , 108.5,\n",
       "       109. , 109.5, 110. , 110.5, 111. , 111.5, 112. , 112.5, 113. ,\n",
       "       113.5, 114. , 114.5, 115. , 115.5, 116. , 116.5, 117. , 117.5,\n",
       "       118. , 118.5, 119. , 119.5, 120. , 120.5, 121. , 121.5, 122. ,\n",
       "       122.5, 123. , 123.5, 124. , 124.5, 125. , 125.5, 126. , 126.5,\n",
       "       127. , 127.5, 128. , 128.5, 129. , 129.5, 130. , 130.5, 131. ,\n",
       "       131.5, 132. , 132.5, 133. , 133.5, 134. , 134.5, 135. , 135.5,\n",
       "       136. , 136.5, 137. , 137.5, 138. , 138.5, 139. , 139.5, 140. ,\n",
       "       140.5, 141. , 141.5, 142. , 142.5, 143. , 143.5, 144. , 144.5,\n",
       "       145. , 145.5, 146. , 146.5, 147. , 147.5, 148. , 148.5, 149. ,\n",
       "       149.5, 150. , 150.5, 151. , 151.5, 152. , 152.5, 153. , 153.5,\n",
       "       154. , 154.5, 155. , 155.5, 156. , 156.5, 157. , 157.5, 158. ,\n",
       "       158.5, 159. , 159.5, 160. , 160.5, 161. , 161.5, 162. , 162.5,\n",
       "       163. , 163.5, 164. , 164.5, 165. , 165.5, 166. , 166.5, 167. ,\n",
       "       167.5, 168. , 168.5, 169. , 169.5, 170. , 170.5, 171. , 171.5,\n",
       "       172. , 172.5, 173. , 173.5, 174. , 174.5, 175. , 175.5, 176. ,\n",
       "       176.5, 177. , 177.5, 178. , 178.5, 179. , 179.5, 180. , 180.5,\n",
       "       181. , 181.5, 182. , 182.5, 183. , 183.5, 184. , 184.5, 185. ,\n",
       "       185.5, 186. , 186.5, 187. , 187.5, 188. , 188.5, 189. , 189.5,\n",
       "       190. , 190.5, 191. , 191.5, 192. , 192.5, 193. , 193.5, 194. ,\n",
       "       194.5, 195. , 195.5, 196. , 196.5, 197. , 197.5, 198. , 198.5,\n",
       "       199. , 199.5, 200. , 200.5, 201. , 201.5, 202. , 202.5, 203. ,\n",
       "       203.5, 204. , 204.5, 205. , 205.5, 206. , 206.5, 207. , 207.5,\n",
       "       208. , 208.5, 209. , 209.5, 210. , 210.5, 211. , 211.5, 212. ,\n",
       "       212.5, 213. , 213.5, 214. , 214.5, 215. , 215.5, 216. , 216.5,\n",
       "       217. , 217.5, 218. , 218.5, 219. , 219.5, 220. , 220.5, 221. ,\n",
       "       221.5, 222. , 222.5, 223. , 223.5, 224. , 224.5, 225. , 225.5,\n",
       "       226. , 226.5, 227. , 227.5, 228. , 228.5, 229. , 229.5, 230. ,\n",
       "       230.5, 231. , 231.5, 232. , 232.5, 233. , 233.5, 234. , 234.5,\n",
       "       235. , 235.5, 236. , 236.5, 237. , 237.5, 238. , 238.5, 239. ,\n",
       "       239.5, 240. , 240.5, 241. , 241.5, 242. , 242.5, 243. , 243.5,\n",
       "       244. , 244.5, 245. , 245.5, 246. , 246.5, 247. , 247.5, 248. ,\n",
       "       248.5, 249. , 249.5, 250. , 250.5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2270.9734742812684\n",
      "-2300.0694120185362\n",
      "-2305.033144809954\n",
      "-2305.3592866667277\n",
      "-2305.363440559174\n",
      "-2305.3634422023033\n",
      "Wall time: 234 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  1.25,   1.25,   2.25,   2.25,   3.25,   3.25,   4.25,   4.25,\n",
       "          5.25,   5.25,   6.25,   6.25,   7.25,   7.25,   8.25,   8.25,\n",
       "          9.25,   9.25,  10.25,  10.25,  11.25,  11.25,  12.25,  12.25,\n",
       "         13.25,  13.25,  14.25,  14.25,  15.25,  15.25,  16.25,  16.25,\n",
       "         17.25,  17.25,  18.25,  18.25,  19.25,  19.25,  20.25,  20.25,\n",
       "         21.25,  21.25,  22.25,  22.25,  23.25,  23.25,  24.25,  24.25,\n",
       "         25.25,  25.25,  26.25,  26.25,  27.25,  27.25,  28.25,  28.25,\n",
       "         29.25,  29.25,  30.25,  30.25,  31.25,  31.25,  32.25,  32.25,\n",
       "         33.25,  33.25,  34.25,  34.25,  35.25,  35.25,  36.25,  36.25,\n",
       "         37.25,  37.25,  38.25,  38.25,  39.25,  39.25,  40.25,  40.25,\n",
       "         41.25,  41.25,  42.25,  42.25,  43.25,  43.25,  44.25,  44.25,\n",
       "         45.25,  45.25,  46.25,  46.25,  47.25,  47.25,  48.25,  48.25,\n",
       "         49.25,  49.25,  50.25,  50.25, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75, 150.75,\n",
       "        150.75, 150.75, 150.75, 150.75]), 4.547473508864641e-13, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#res=NewtonMethod(A, x0, a, b, tol)\n",
    "NewtonMethod(A, x0, alpha, beta, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.909594e+01\n",
       "1    4.963733e+00\n",
       "2    3.261419e-01\n",
       "3    4.153892e-03\n",
       "4    1.643130e-06\n",
       "5    4.547474e-13\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts=pd.Series(epsilon_hist)\n",
    "plt=ts.plot(logy=True)\n",
    "plt.set_xlabel('iteration')\n",
    "plt.set_ylabel('epsilon')\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 3.099076550938662e-12,\n",
       " 5.9821037012852685e-12,\n",
       " 2.2035706592760107e-12,\n",
       " 1.0793810290010697e-11,\n",
       " 1.5482060078397808e-11]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to show the algorithm enforces the constraints correctly,\n",
    "# we show l1-norm of x_hist[0] with every elem of x_hist[0..n] are zero\n",
    "[sum(A@(x_hist[0] - x)) for x in x_hist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "\n",
    "First, show the equivelent between these 4 conditions.\n",
    "___\n",
    "* Condition 1 is equivalent to Condition 2\n",
    "\n",
    "Condition 1 $\\rightarrow$ Condition 2\n",
    "\n",
    "Suppose $x \\in \\mathcal N(A) \\cap \\mathcal N(H)$ and $x \\neq 0$. In condition 2, we have  $Ax = 0\\, x \\neq 0$. Since $x \\in \\mathcal N(A) \\cap \\mathcal N(H)$, we have $x^THx = 0$, this contradict to to second statement.\n",
    "\n",
    "Condition 2 $\\rightarrow$ Condition 1\n",
    "\n",
    "If there is an $x$ s.t. $Ax = 0\\, x \\neq 0, x^THX = 0$. Because $H$ is semipositive definate, we must have $Hx = 0$, i.e.   $x \\in \\mathcal N(A) \\cap \\mathcal N(H)$\n",
    "___\n",
    "* Condition 2 is equivalent to Condition 3\n",
    "\n",
    "If $Ax = 0, x \\neq 0$, then because $\\mathcal R(F) = \\mathcal N (A)$, we have $x = Fz$ and $z \\neq 0$. Then because of 2 we have $x^T Hx = z^TF^THFZ > 0$\n",
    "\n",
    "___\n",
    "* Condition 2 is equivalent to Condition 4\n",
    "If we have the condition 2. then \n",
    "\n",
    "$$\n",
    "x^T(H + A^TA) = x^THx + \\vert\\vert A^Tx\\vert\\vert^2_2 > 0\n",
    "$$\n",
    "for all nonzero x, sot the condition 4 holds for $Q = I$.\n",
    "If the fourth condition hols with general semipositive definate form of $Q$.\n",
    "$$\n",
    "x^T(H + A^TQA) = x^THx + x^TA^TQAx > 0\n",
    "$$\n",
    "for all nonzero $x$, Therefore if $Ax = 0\\, x \\neq 0$, we must have $x\\neq 0$\n",
    "___\n",
    "Second show these four conditions are equivalent to nonsigularity of KKT matrix.\n",
    "\n",
    "Suppose $x\\neq 0$, s.t.$Ax = 0, Hx = 0$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "H & A^T\\\\\n",
    "A & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "0\n",
    "\\end{bmatrix} = 0$$\n",
    "\n",
    "If the KKT matrix is singular, $x,z$ are not both zero, and we have\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "H & A^T\\\\\n",
    "A & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "z\n",
    "\\end{bmatrix} = 0$$\n",
    "\n",
    "This leads to $Hx + A^Tz = 0$, and $Ax = 0$. \n",
    "from the first equation, we have \n",
    "$$\n",
    "x^THx + x^TA^Tz = 0.\n",
    "$$\n",
    "plug in the second equation.\n",
    "we have $x^THx = 0$, this contradicts second condition unless $x= 0$.\n",
    "\n",
    "## (b)\n",
    "From previous question, we have $H + A^TA$ is positive definate matrix. Then we must hvae $R \\in \\mathbb R^{n\\times n}$, s.t.\n",
    "$$\n",
    "R^T(H + A^TA)R = I\n",
    "$$\n",
    "Apply SVD to $AR$, $AR = U\\Sigma V_1^T$, where $\\Sigma = diag(\\sigma_1,\\cdots, \\sigma_p)$. Denote $V_2 \\in \\mathbb R^{n\\times(n-p)}$, s.t.\n",
    "$$\n",
    "V = \\left[V_1\\quad V_2\\right]\n",
    "$$\n",
    "$V_2$ is orthogonal, and let \n",
    "$$\n",
    "S = \\left[\\Sigma\\quad 0\\right] \\in \\mathbb R^{p\\times n}\n",
    "$$\n",
    "We have $AR = USV^T$, then \n",
    "$$\n",
    "V^TR^T(H + A^TA)RV = V^TR^THRV + S^TS = I\n",
    "$$\n",
    "Becasue $V^TR^THRV = I - S^TS$ is diagonal matrix, then denote it as $D$\n",
    "$$\n",
    "D = V^TR^THRV = diag(1-\\sigma_1^2, \\cdots, 1-\\sigma_p^2, 1, \\cdots, 1)\n",
    "$$\n",
    "Apply the transform to KKT matrix, we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "V^TR^T & 0\\\\\n",
    "0 & U\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "H & A^T\\\\\n",
    "A & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "RV & 0\\\\\n",
    "0 & U\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "D & S^T\\\\\n",
    "S & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Apply the permutation to hte matrix on the right gives a block diaonal matrix with n diagonal blocks.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\lambda_i & \\sigma_i\\\\\n",
    "\\sigma_i & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $i = 1,2,\\cdots, p$, \n",
    "\n",
    "and $\\lambda_i = 1$ for $i = p +1, \\cdots, n$\n",
    "\n",
    "This matrix have eiggenvalues of $\\frac{\\lambda_i \\pm \\sqrt{\\lambda_i^2 + 4\\sigma_i^2}}{2}$, i.e. one eigen value is positive, another is negative. \n",
    "\n",
    "In total, there are n positive evalues and p negative evalues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
